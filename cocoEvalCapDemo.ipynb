{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b921cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pycocotools.coco import COCO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21c48c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocoevalcap.eval import COCOEvalCap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eb474da",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "import json\n",
    "from json import encoder\n",
    "encoder.FLOAT_REPR = lambda o: format(o, '.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e4bed9c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# set up file names and pathes\n",
    "dataDir='./'\n",
    "dataType='val2014'\n",
    "algName = 'fakecap'\n",
    "annFile='%s/annotations/captions_%s.json'%(dataDir,dataType)\n",
    "subtypes=['results', 'evalImgs', 'eval']\n",
    "[resFile, evalImgsFile, evalFile]= \\\n",
    "['%s/results/captions_%s_%s_%s.json'%(dataDir,dataType,algName,subtype) for subtype in subtypes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3445c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download Stanford models\n",
    "# !get_stanford_models.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d6c2c64",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.303162\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# create coco object and cocoRes object\n",
    "coco = COCO(annFile)\n",
    "cocoRes = coco.loadRes(resFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7439416",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# create cocoEval object by taking coco and cocoRes\n",
    "cocoEval = COCOEvalCap(coco, cocoRes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95b756b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on a subset of images by setting\n",
    "# cocoEval.params['image_id'] = cocoRes.getImgIds()\n",
    "# please remove this line when evaluating the full validation set\n",
    "cocoEval.params['image_id'] = cocoRes.getImgIds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dc2b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenization...\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 9893, 'reflen': 9855, 'guess': [9893, 8893, 7893, 6893], 'correct': [5732, 2510, 1043, 423]}\n",
      "ratio: 1.003855910705124\n",
      "Bleu_1: 0.579\n",
      "Bleu_2: 0.404\n",
      "Bleu_3: 0.279\n",
      "Bleu_4: 0.191\n",
      "computing METEOR score...\n"
     ]
    }
   ],
   "source": [
    "# evaluate results\n",
    "# SPICE will take a few minutes the first time, but speeds up due to caching\n",
    "cocoEval.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01d64d58",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cocoEval' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# print output evaluation scores\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m metric, score \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcocoEval\u001b[49m.eval.items():\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m (\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[33m'\u001b[39m%(metric, score))\n",
      "\u001b[31mNameError\u001b[39m: name 'cocoEval' is not defined"
     ]
    }
   ],
   "source": [
    "# print output evaluation scores\n",
    "for metric, score in cocoEval.eval.items():\n",
    "    print ('%s: %.3f'%(metric, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b34ac2f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cocoEval' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# demo how to use evalImgs to retrieve low score result\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m evals = [eva \u001b[38;5;28;01mfor\u001b[39;00m eva \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcocoEval\u001b[49m.evalImgs \u001b[38;5;28;01mif\u001b[39;00m eva[\u001b[33m'\u001b[39m\u001b[33mCIDEr\u001b[39m\u001b[33m'\u001b[39m]<\u001b[32m30\u001b[39m]\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m (\u001b[33m'\u001b[39m\u001b[33mground truth captions\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m imgId = evals[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mimage_id\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'cocoEval' is not defined"
     ]
    }
   ],
   "source": [
    "# demo how to use evalImgs to retrieve low score result\n",
    "evals = [eva for eva in cocoEval.evalImgs if eva['CIDEr']<30]\n",
    "print ('ground truth captions')\n",
    "imgId = evals[0]['image_id']\n",
    "annIds = coco.getAnnIds(imgIds=imgId)\n",
    "anns = coco.loadAnns(annIds)\n",
    "coco.showAnns(anns)\n",
    "\n",
    "print ('\\n')\n",
    "print ('generated caption (CIDEr score %0.1f)'%(evals[0]['CIDEr']))\n",
    "annIds = cocoRes.getAnnIds(imgIds=imgId)\n",
    "anns = cocoRes.loadAnns(annIds)\n",
    "coco.showAnns(anns)\n",
    "\n",
    "img = coco.loadImgs(imgId)[0]\n",
    "I = io.imread('%s/images/%s/%s'%(dataDir,dataType,img['file_name']))\n",
    "plt.imshow(I)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853545dc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# plot score histogram\n",
    "ciderScores = [eva['CIDEr'] for eva in cocoEval.evalImgs]\n",
    "plt.hist(ciderScores)\n",
    "plt.title('Histogram of CIDEr Scores', fontsize=20)\n",
    "plt.xlabel('CIDEr score', fontsize=20)\n",
    "plt.ylabel('result counts', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44895a27",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# save evaluation results to ./results folder\n",
    "json.dump(cocoEval.evalImgs, open(evalImgsFile, 'w'))\n",
    "json.dump(cocoEval.eval,     open(evalFile, 'w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
